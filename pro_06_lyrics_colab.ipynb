{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"pro_06_lyrics_colab.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-t7BdR0Yqlx4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m8C1mB1SQ8sh"},"source":["# 1. 요약 : 절차\n","  \n","  - 데이터 준비\n","    - 노랫말(song lyrics) 데이터 받기\n","    - 데이터 크기: 187088  \n","      \n","      \n","\n","  - 데이터 분석 / 전처리\n","    - data\n","      - 문장 길이가 0인 데이터 제외\n","      - 소문자로 변경\n","      - 특수문자 및 공백 제외\n","      - 문장의 시작과 끝을 표시 : <start> + 문장 + <end>  \n","    - tokenize\n","      - 구축한 corpus를 tokenize  \n","    - 데이터 : source, target = 140599, 140599\n","      - total params = 22,607,961  \n","      \n","    \n","    \n","  - 학습 : Optimizer를 세가지로 학습\n","    - Adam optimizer : loss = 1.2485\n","    - Adadelta optimizer : loss = 1.1361\n","    - Nadam optimizer : loss = 0.9800\n","    - 학습한 것을 다시 학습시켜서인지 loss 감소  \n","      \n","    \n","    \n","  - 학습한 결과를 바탕으로 text 생성\n","    \n","result|Adam|Adade|Nadam\n","---|---|---|---\n","i love|i love the the for so you you to go|i love the the for into you you|i love the the you i in that had to be for\n","you love|you love future the so is road that far|you love future the so is road that far|you love future a same that know me around\n","\n","    \n","    \n"]},{"cell_type":"code","metadata":{"id":"YK_-lphRqlKv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"StDm6V-GRAZN"},"source":["# 2. 회고\n","\n","  - 이번 과제에서도 어김없이 오류가 발생 - 컴에 문제가 있는 듯\n","    - 주피터 노트북 재설치 해볼 예정  \n","    \n","    \n","  - 다른 과제처럼 그것을 해결하기 위해 노력하기 보다 일단 결과물을 먼저 도출해내자는 생각\n","    - 구글 코랩에 바로 접속해서 실행시킴.\n","    - 결과물을 도출하고 여유도 좀 있어서 Optimizer를 바꿔가면서 결과를 지켜봄.\n","    - Optimizer에 따라 결과가 다르게 나옴.  \n","    \n","    \n","  - 토큰화 하는 부분과 텍스트를 만드는 부분에서 학습이 좀더 필요함.\n","    - 아직 학습해야할 것이 많음. \n","    - 4주차가 되면서 조금 안일한 모습이 보이는 것 같아 다시 초심으로 돌아가자고 다짐.\n","      - 노드가 밀리지 않도록 열심히 하자고 다짐.\n","      - 코딩 부분을 좀더 살펴보면서 로직을 이해해야 함. 로직을 모른 채 옮기기만 하는 모습이 비춰짐.\n","      - 최대한 로직을 학습하고 구글링 하는 노력도 필요.  \n","        \n","        \n","  - 머신러닝을 학습하면서 눈으로만 학습했는데, 아이펠에서는 코딩을 하니 실력이 예전보다 늘어났음을 느낄 수 있음\n","  "]},{"cell_type":"code","metadata":{"id":"S1vHo5rwfytv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VgsMEWiQfytz"},"source":["import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n","import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n","import tensorflow as tf    # 대망의 텐서플로우!\n","import os\n","import sklearn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdD7DSnBg_vg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TClQ1egBjwVR","executionInfo":{"status":"ok","timestamp":1611809974933,"user_tz":-540,"elapsed":20821,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"5b4a02f3-30b4-42aa-fef0-c976d10d6374"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Odl1IPG1hCaU"},"source":["data_path = '/content/drive/MyDrive/Colab Notebooks/ssac/project/pro_06_lyrics/lyrics_data/*.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbQRDXmnfyt1","executionInfo":{"status":"ok","timestamp":1611810027128,"user_tz":-540,"elapsed":16973,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"b98c0799-89c4-462d-fca1-872c25d202d9"},"source":["import glob\n","import pandas as pd\n","\n","#txt_file_path = os.getenv(data_path)\n","txt_list = glob.glob(data_path)\n","\n","raw_corpus = []\n","\n","# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n","for txt_file in txt_list:\n","    with open(txt_file, \"r\") as f:\n","        raw = f.read().splitlines()\n","        raw_corpus.extend(raw)\n","\n","print(\"데이터 크기:\", len(raw_corpus))\n","print(\"Examples:\\n\", raw_corpus[:3])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["데이터 크기: 187088\n","Examples:\n"," [\"I'll undress you, 'cause you're tired\", 'Cover you as you desire', 'When you fall asleep inside my arms']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kz04wiKNfyt1","executionInfo":{"status":"ok","timestamp":1611810033604,"user_tz":-540,"elapsed":722,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"906743f3-3131-4052-baab-6e8ed1bccc24"},"source":["for idx, sentence in enumerate(raw_corpus):\n","    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n","    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n","\n","    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n","        \n","    print(sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["I'll undress you, 'cause you're tired\n","Cover you as you desire\n","When you fall asleep inside my arms\n","May not have the fancy things\n","But I'll give you everything\n","You could ever want, it's in my arms So baby tell me yes\n","And I will give you everything\n","So baby tell me yes\n","And I will be all yours tonight\n","So baby tell me yes\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OQkqqYkmfyt2","executionInfo":{"status":"ok","timestamp":1611810036058,"user_tz":-540,"elapsed":627,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"559a3e29-2f45-4c0f-e3dc-826a5ce64a9e"},"source":["def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n","  \n","    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n","    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n","\n","    sentence = sentence.strip()\n","\n","    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n","    \n","    return sentence\n","\n","print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."],"execution_count":null,"outputs":[{"output_type":"stream","text":["<start> this is sample sentence . <end>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5oUMJ6sfyt2","executionInfo":{"status":"ok","timestamp":1611810039949,"user_tz":-540,"elapsed":2161,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"3afb1390-5f92-4c29-e00d-b307e287a517"},"source":["corpus = []\n","\n","for sentence in raw_corpus:\n","    if len(sentence) == 0: continue\n","    if sentence[-1] == \":\": continue\n","        \n","    corpus.append(preprocess_sentence(sentence))\n","        \n","corpus[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> i ll undress you , cause you re tired <end>',\n"," '<start> cover you as you desire <end>',\n"," '<start> when you fall asleep inside my arms <end>',\n"," '<start> may not have the fancy things <end>',\n"," '<start> but i ll give you everything <end>',\n"," '<start> you could ever want , it s in my arms so baby tell me yes <end>',\n"," '<start> and i will give you everything <end>',\n"," '<start> so baby tell me yes <end>',\n"," '<start> and i will be all yours tonight <end>',\n"," '<start> so baby tell me yes <end>']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WezlzlZTfyt3","executionInfo":{"status":"ok","timestamp":1611810050381,"user_tz":-540,"elapsed":4117,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"f417b9fe-cffa-45c8-d9de-345430b4f21c"},"source":["def tokenize(corpus):\n","    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=7000,  # 전체 단어의 개수 \n","        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n","        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n","    )\n","    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n","\n","    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n","    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n","\n","    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n","    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n","\n","    \n","    #jjang : maxlen = 15로 조정\n","    #tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15, padding='post')\n","\n","    print(tensor,tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[   2    5   61 ...    0    0    0]\n"," [   2 1169    7 ...    0    0    0]\n"," [   2   46    7 ...    0    0    0]\n"," ...\n"," [   2  100    6 ...    0    0    0]\n"," [   2  122    9 ...    0    0    0]\n"," [   2   74    6 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f61c37825f8>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-u_BDmufyt3","executionInfo":{"status":"ok","timestamp":1611810064565,"user_tz":-540,"elapsed":886,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"bdeef85f-799f-44db-9353-e73a37efe111"},"source":["print(tensor[:3, :10])\n","\n","print(type(tensor), len(tensor))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[   2    5   61 3121    7    4   67    7   54  617]\n"," [   2 1169    7   81    7 1255    3    0    0    0]\n"," [   2   46    7  292 1599  294   13  465    3    0]]\n","<class 'numpy.ndarray'> 175749\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WgJv7KDZfyt3","executionInfo":{"status":"ok","timestamp":1611810066602,"user_tz":-540,"elapsed":804,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"61c988f7-8cdc-4b67-be06-3ecda2478395"},"source":["for idx in tokenizer.index_word:\n","    print(idx, \":\", tokenizer.index_word[idx])\n","\n","    if idx >= 10: break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : ,\n","5 : i\n","6 : the\n","7 : you\n","8 : and\n","9 : a\n","10 : to\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_3XTUSQHfyt4","executionInfo":{"status":"ok","timestamp":1611810070047,"user_tz":-540,"elapsed":595,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"32148e41-142f-472f-d27b-bfbcde7e5f90"},"source":["src_input = tensor[:-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n","tgt_input = tensor[1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n","\n","print(src_input[0])\n","print(tgt_input[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[   2    5   61 3121    7    4   67    7   54  617    3    0    0    0\n","    0]\n","[   2 1169    7   81    7 1255    3    0    0    0    0    0    0    0\n","    0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_h4X0l9rfyt4","executionInfo":{"status":"ok","timestamp":1611810337561,"user_tz":-540,"elapsed":601,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"40b5c2d8-300b-4a15-f1b4-71498fdd6c32"},"source":["#jjang : 데이터 분할 \n","\n","from sklearn.model_selection import train_test_split\n","\n","enc_train, enc_valid, dec_train, dec_valid= train_test_split(tensor, tensor, test_size=0.2, shuffle=True)\n","\n","print(\"Source Train : \", enc_train.shape)\n","print(\"Target Train : \", dec_train.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Source Train :  (140599, 15)\n","Target Train :  (140599, 15)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sFY1joOfyt4","executionInfo":{"status":"ok","timestamp":1611810466603,"user_tz":-540,"elapsed":570,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"4aba31e5-857e-41b5-ff8c-c764c0aa7d46"},"source":["BUFFER_SIZE = len(src_input)\n","BATCH_SIZE = 256\n","steps_per_epoch = len(src_input) // BATCH_SIZE\n","\n","VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","\n","dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((256, 15), (256, 15)), types: (tf.int32, tf.int32)>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"Y0cPALCEfyt5"},"source":["class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super(TextGenerator, self).__init__()\n","        \n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","        \n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.linear(out)\n","        \n","        return out\n","    \n","embedding_size = 256\n","hidden_size = 1024\n","model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6q4mNePUfyt6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611810481410,"user_tz":-540,"elapsed":8759,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"044ae994-2d94-4ca5-eab1-0736e70ee2b0"},"source":["for src_sample, tgt_sample in dataset.take(1): break\n","model(src_sample)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(256, 15, 7001), dtype=float32, numpy=\n","array([[[ 4.09330642e-05,  1.37340576e-05,  1.61856224e-04, ...,\n","          1.63501900e-04, -1.60792755e-04,  1.24444836e-04],\n","        [ 1.81842814e-04,  5.21840004e-04,  4.39726457e-04, ...,\n","         -1.76459580e-04, -3.33700591e-04,  1.80720526e-04],\n","        [ 2.93316174e-04,  6.31218660e-04,  4.60614188e-04, ...,\n","         -4.90534236e-04, -5.03324030e-04,  3.71948758e-04],\n","        ...,\n","        [ 6.80436613e-04,  5.24469768e-04,  1.39803218e-03, ...,\n","          1.09534257e-03, -1.60717114e-03,  8.17703200e-04],\n","        [ 9.22679203e-04,  5.15878666e-04,  1.72102207e-03, ...,\n","          1.37571990e-03, -1.74075901e-03,  7.95388594e-04],\n","        [ 1.20978907e-03,  5.40311972e-04,  2.01766216e-03, ...,\n","          1.59502355e-03, -1.89046061e-03,  7.71301333e-04]],\n","\n","       [[ 4.09330642e-05,  1.37340576e-05,  1.61856224e-04, ...,\n","          1.63501900e-04, -1.60792755e-04,  1.24444836e-04],\n","        [-3.04567046e-04,  1.07314750e-04,  2.14887812e-04, ...,\n","          5.38904977e-04, -1.79877825e-04,  7.32910121e-05],\n","        [-3.46710091e-04,  2.78189691e-05,  9.97361640e-05, ...,\n","          7.61229196e-04, -3.46005254e-04, -5.90794443e-05],\n","        ...,\n","        [ 1.56556221e-03,  6.74919982e-04,  3.10069369e-03, ...,\n","          2.81143817e-03, -2.95058987e-03,  2.71246769e-04],\n","        [ 1.86910573e-03,  7.55350338e-04,  3.31067387e-03, ...,\n","          2.81777605e-03, -3.08018434e-03,  3.25630594e-04],\n","        [ 2.13936064e-03,  8.15940264e-04,  3.49432416e-03, ...,\n","          2.82194139e-03, -3.19331372e-03,  3.74306925e-04]],\n","\n","       [[-2.14559477e-05, -6.57747441e-05,  2.91426724e-04, ...,\n","         -7.29822132e-05,  6.10978022e-05,  1.41278260e-05],\n","        [-1.25697654e-04,  1.25424282e-04,  2.82584835e-04, ...,\n","         -2.12821600e-04, -7.43273340e-05, -3.53578143e-05],\n","        [-6.92063913e-05,  1.32489076e-04, -7.50100080e-05, ...,\n","         -2.23966403e-04, -6.86415078e-05, -1.17142787e-04],\n","        ...,\n","        [ 4.25358623e-04,  1.23587728e-03,  9.55466588e-04, ...,\n","         -1.07649190e-03, -2.71458528e-04,  1.13914255e-04],\n","        [ 8.91721924e-04,  1.29629555e-03,  7.77005975e-04, ...,\n","         -1.44118397e-03, -3.37997859e-04,  5.04353549e-04],\n","        [ 9.58725577e-04,  8.67159164e-04,  7.50158331e-04, ...,\n","         -1.31420488e-03, -2.64950213e-04,  6.85826992e-04]],\n","\n","       ...,\n","\n","       [[ 4.09330642e-05,  1.37340576e-05,  1.61856224e-04, ...,\n","          1.63501900e-04, -1.60792755e-04,  1.24444836e-04],\n","        [ 4.47207945e-04,  1.76695234e-04, -1.79090697e-04, ...,\n","          1.46293445e-04, -1.65738093e-04,  1.24346625e-04],\n","        [ 9.74421331e-04,  4.37742070e-04, -7.10757566e-04, ...,\n","         -2.71073222e-05, -1.04747458e-04,  1.15306539e-04],\n","        ...,\n","        [-2.68069241e-04,  1.49621489e-03, -3.22151557e-03, ...,\n","         -4.32156114e-04, -1.38599216e-03,  7.89718644e-04],\n","        [-3.14779143e-04,  1.09088316e-03, -3.14223650e-03, ...,\n","         -9.50989706e-05, -1.10714312e-03,  7.41779397e-04],\n","        [-3.27855931e-04,  8.03334930e-04, -2.68305442e-03, ...,\n","          3.68303445e-04, -1.09434803e-03,  7.16365990e-04]],\n","\n","       [[ 4.09330642e-05,  1.37340576e-05,  1.61856224e-04, ...,\n","          1.63501900e-04, -1.60792755e-04,  1.24444836e-04],\n","        [ 5.16431210e-05,  2.18550449e-05,  1.02419835e-04, ...,\n","          4.76412970e-04, -5.84472844e-04,  1.48368330e-04],\n","        [ 2.43819610e-04,  2.46459327e-04,  4.54643377e-05, ...,\n","          9.08863207e-04, -1.11369649e-03,  1.11851681e-04],\n","        ...,\n","        [ 8.52098048e-04, -1.82626047e-03,  7.27447448e-04, ...,\n","          2.48451205e-03, -1.60161231e-03,  3.92837625e-04],\n","        [ 1.07140746e-03, -1.49244571e-03,  1.17624586e-03, ...,\n","          2.55014119e-03, -1.82269141e-03,  3.94739356e-04],\n","        [ 1.32149516e-03, -1.14890316e-03,  1.58745633e-03, ...,\n","          2.57044076e-03, -2.04448868e-03,  3.97449359e-04]],\n","\n","       [[ 4.09330642e-05,  1.37340576e-05,  1.61856224e-04, ...,\n","          1.63501900e-04, -1.60792755e-04,  1.24444836e-04],\n","        [ 3.62303072e-05, -1.87548270e-04,  3.01214255e-04, ...,\n","          1.78270333e-04,  1.07936743e-04,  3.13939265e-04],\n","        [ 2.29621637e-05, -2.79183296e-04,  2.41839836e-04, ...,\n","         -2.88785115e-04,  2.50393554e-04,  2.04177759e-05],\n","        ...,\n","        [-4.91645660e-05, -3.08017305e-04,  2.09122407e-03, ...,\n","          7.54582754e-04, -1.41461380e-03, -9.01915264e-05],\n","        [ 3.50770308e-04, -1.48631356e-04,  2.40803510e-03, ...,\n","          1.07146811e-03, -1.64187793e-03,  6.95135750e-05],\n","        [ 7.47865823e-04,  1.14369550e-05,  2.68408027e-03, ...,\n","          1.33652543e-03, -1.86640094e-03,  2.00423849e-04]]],\n","      dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"dOjmfbC9fyt6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611810493616,"user_tz":-540,"elapsed":1043,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"3df869a3-51fc-434b-f1b6-653b67afdba5"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"text_generator\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        multiple                  1792256   \n","_________________________________________________________________\n","lstm (LSTM)                  multiple                  5246976   \n","_________________________________________________________________\n","lstm_1 (LSTM)                multiple                  8392704   \n","_________________________________________________________________\n","dense (Dense)                multiple                  7176025   \n","=================================================================\n","Total params: 22,607,961\n","Trainable params: 22,607,961\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8d0xldnffyt6"},"source":["# from tensorflow.keras import backend as K\n","# from tensorflow.keras.layers import *\n","# from tensorflow.keras.models import *\n","# from tensorflow.keras.callbacks import * "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yitNxZbufyt6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611813560386,"user_tz":-540,"elapsed":1104103,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"d6c8c56f-f40e-463a-c598-aebf4ca92d19"},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,\n","    reduction='none'\n",")\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","model.fit(dataset, epochs=10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","686/686 [==============================] - 111s 159ms/step - loss: 1.8783\n","Epoch 2/10\n","686/686 [==============================] - 110s 160ms/step - loss: 1.7531\n","Epoch 3/10\n","686/686 [==============================] - 110s 160ms/step - loss: 1.6654\n","Epoch 4/10\n","686/686 [==============================] - 110s 160ms/step - loss: 1.5945\n","Epoch 5/10\n","686/686 [==============================] - 111s 161ms/step - loss: 1.5215\n","Epoch 6/10\n","686/686 [==============================] - 110s 161ms/step - loss: 1.4558\n","Epoch 7/10\n","686/686 [==============================] - 110s 161ms/step - loss: 1.3940\n","Epoch 8/10\n","686/686 [==============================] - 111s 161ms/step - loss: 1.3388\n","Epoch 9/10\n","686/686 [==============================] - 110s 161ms/step - loss: 1.2951\n","Epoch 10/10\n","686/686 [==============================] - 111s 161ms/step - loss: 1.2485\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f6161abc438>"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"UK9h2PzWfyt7"},"source":["def generate_text(model, tokenizer, init_sentence=\"<start> \", max_len=20):\n","    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","    end_token = tokenizer.word_index[\"<end>\"]\n","\n","    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n","    while True:\n","        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n","\n","        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n","        test_tensor = tf.concat([test_tensor, \n","                    tf.expand_dims(predict_word, axis=0)], axis=-1)\n","\n","        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = \"\"\n","    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","\n","    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rTUL5gtdfyt7","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1611814919582,"user_tz":-540,"elapsed":929,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"8d47f692-dbb3-4049-8e5d-54e285199aca"},"source":["generate_text(model, tokenizer, init_sentence=\"<start>i i love\")\n","# <start> i love\"는 오류발생\n","# <start>i love\"는 'i'가 누락"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<unk> i love the the for so you you to go <end> '"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"oUKsafPup9m_","executionInfo":{"status":"ok","timestamp":1611814943873,"user_tz":-540,"elapsed":597,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"0abc406e-6bc6-445e-f3e8-a8391715e3bc"},"source":["generate_text(model, tokenizer, init_sentence=\"<start>i love\")\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<unk> love i just all all back head me head head <end> '"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"Bs9bMScXfyt7","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1611814947861,"user_tz":-540,"elapsed":640,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"19dc4272-2038-4386-ec8a-649ecb048c80"},"source":["generate_text(model, tokenizer, init_sentence=\"<start>you love\")\n","# generate_text(model, tokenizer, init_sentence=\"<start>i love\")와 결과 같음\n","# <start>you love\" : you를 인지하지 못함"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<unk> love i just all all back head me head head <end> '"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"WTGuixLufyt8","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1611817972805,"user_tz":-540,"elapsed":666,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"4c3f99de-40f6-4b00-9923-51fb330e36a0"},"source":["generate_text(model, tokenizer, init_sentence=\"<start>you you love\")\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<unk> you love future the so is road that far <end> '"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"YL1R3qodfyt8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JVyR8O8vfyt8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0bSeLE7w_tJ","executionInfo":{"status":"ok","timestamp":1611817887650,"user_tz":-540,"elapsed":1048564,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"bf3527a9-a15b-4740-d5d4-b87ac2f572c5"},"source":["optimizer = tf.keras.optimizers.Adadelta()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,\n","    reduction='none'\n",")\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","model.fit(dataset, epochs=10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","686/686 [==============================] - 108s 154ms/step - loss: 1.1693\n","Epoch 2/10\n","686/686 [==============================] - 104s 152ms/step - loss: 1.1600\n","Epoch 3/10\n","686/686 [==============================] - 104s 152ms/step - loss: 1.1603\n","Epoch 4/10\n","686/686 [==============================] - 104s 152ms/step - loss: 1.1521\n","Epoch 5/10\n","686/686 [==============================] - 105s 152ms/step - loss: 1.1497\n","Epoch 6/10\n","686/686 [==============================] - 104s 152ms/step - loss: 1.1443\n","Epoch 7/10\n","686/686 [==============================] - 104s 152ms/step - loss: 1.1454\n","Epoch 8/10\n","686/686 [==============================] - 104s 152ms/step - loss: 1.1420\n","Epoch 9/10\n","686/686 [==============================] - 105s 152ms/step - loss: 1.1363\n","Epoch 10/10\n","686/686 [==============================] - 104s 152ms/step - loss: 1.1361\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f6161abc8d0>"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"dNk-CA3Dfyt8","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1611818704465,"user_tz":-540,"elapsed":770,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"f27ffc5a-2ed5-4709-c50e-9d4b739188f1"},"source":["generate_text(model, tokenizer, init_sentence=\"<start>i i love\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<unk> i love the the for into you you <end> '"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"JBTdPljzfyt8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"DIax4WE84KO7","executionInfo":{"status":"ok","timestamp":1611818710007,"user_tz":-540,"elapsed":765,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"6cffe51c-dba8-44b3-9a42-04e82c5abb9c"},"source":["generate_text(model, tokenizer, init_sentence=\"<start>you you love\")\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<unk> you love future the so is road that far <end> '"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"7W1PXsfUfyt8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wwqRUU5Wfyt9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tAkg7S4v_WTC","executionInfo":{"status":"ok","timestamp":1611821775884,"user_tz":-540,"elapsed":1127887,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"0659f3e9-62a2-4b6f-bba8-37d153de64f0"},"source":["optimizer = tf.keras.optimizers.Nadam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,\n","    reduction='none'\n",")\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","model.fit(dataset, epochs=10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","686/686 [==============================] - 115s 164ms/step - loss: 1.2113\n","Epoch 2/10\n","686/686 [==============================] - 112s 164ms/step - loss: 1.1578\n","Epoch 3/10\n","686/686 [==============================] - 112s 164ms/step - loss: 1.1256\n","Epoch 4/10\n","686/686 [==============================] - 112s 164ms/step - loss: 1.0967\n","Epoch 5/10\n","686/686 [==============================] - 112s 164ms/step - loss: 1.0711\n","Epoch 6/10\n","686/686 [==============================] - 112s 164ms/step - loss: 1.0485\n","Epoch 7/10\n","686/686 [==============================] - 113s 164ms/step - loss: 1.0242\n","Epoch 8/10\n","686/686 [==============================] - 112s 164ms/step - loss: 1.0090\n","Epoch 9/10\n","686/686 [==============================] - 113s 164ms/step - loss: 0.9917\n","Epoch 10/10\n","686/686 [==============================] - 112s 164ms/step - loss: 0.9800\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f616070b908>"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"YGe4WdVW_Ww8","executionInfo":{"status":"ok","timestamp":1611821789055,"user_tz":-540,"elapsed":784,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"1bf456cc-d001-45fb-b176-6d1ac67d9725"},"source":["generate_text(model, tokenizer, init_sentence=\"<start>i i love\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<unk> i love the the you i in that had to be for <end> '"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"aApvSSFKQ6fi"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"h8Ys69El_XUS","executionInfo":{"status":"ok","timestamp":1611821794876,"user_tz":-540,"elapsed":806,"user":{"displayName":"장인학","photoUrl":"","userId":"11585679766809129464"}},"outputId":"6f939fdd-4149-49be-cbbb-52686bf17c4e"},"source":["generate_text(model, tokenizer, init_sentence=\"<start>you you love\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<unk> you love future a same that know me around <end> '"]},"metadata":{"tags":[]},"execution_count":64}]}]}